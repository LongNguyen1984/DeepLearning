{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SaveLoadData.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPgwAeQRP8QDAdQmkjnZ7Oy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LongNguyen1984/DeepLearning/blob/master/SaveLoadData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7HWKgL-SgLQ"
      },
      "source": [
        "# Define Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLi5mWTsQ4eR",
        "outputId": "cc54ae30-08a6-48e6-97b9-9930a626ce0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# example of creating a test dataset and splitting it into train and test sets\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.model_selection import train_test_split\n",
        "# prepare dataset\n",
        "X, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=1)\n",
        "# split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
        "# split data into train and test variable\n",
        "for i in range(X_test.shape[1]):\n",
        "  print('>%d, train: min=%.3f, max=%.3f, test: min=%.3f, max=%.3f'%\n",
        "        (i, X_train[:,i].min(), X_train[:, i].max(),\n",
        "         X_test[:, i].min(), X_test[:, i].max()))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">0, train: min=-11.856, max=0.526, test: min=-11.270, max=0.085\n",
            ">1, train: min=-6.388, max=6.507, test: min=-5.581, max=5.926\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgQ3ckCYSjrv"
      },
      "source": [
        "# Scale the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtVL92NvSUdi",
        "outputId": "1710a1e8-8433-4c5e-ebb2-8229165c7d75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# example of scaling the dataset\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "# prepare dataset\n",
        "X, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=1)\n",
        "# split data into train and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.33, random_state=1)\n",
        "# define scaler\n",
        "scaler = MinMaxScaler()\n",
        "# fit scaler on training dataset\n",
        "scaler.fit(X_train)\n",
        "# transform both datasets\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "# summarize the scale of each input variable\n",
        "for i in range(X_test.shape[1]):\n",
        "  print('>%d, train: min=%.3f, max=%.3f, test: min=%.3f, max=%.3f' %\n",
        "        (i, X_train_scaled[:, i].min(), X_train_scaled[:, i].max(),\n",
        "         X_test_scaled[:, i].min(), X_test_scaled[:, i].max()))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">0, train: min=0.000, max=1.000, test: min=0.047, max=0.964\n",
            ">1, train: min=0.000, max=1.000, test: min=0.063, max=0.955\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odymFw1eVfxV"
      },
      "source": [
        "# Save Model and Data Scaler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3uHlJ6pVfKP"
      },
      "source": [
        "# example of fitting a model on the scaled dataset\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from pickle import dump\n",
        "# prepare dataset\n",
        "X, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=1)\n",
        "# split data into train and test sets\n",
        "X_train, _, y_train, _ = train_test_split(X, y, test_size=0.33, random_state=1)\n",
        "# define scaler\n",
        "scaler = MinMaxScaler()\n",
        "# fit scaler on the training dataset\n",
        "scaler.fit(X_train)\n",
        "# transform the training dataset\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "# define the model\n",
        "model = LogisticRegression(solver='lbfgs')\n",
        "model.fit(X_train_scaled, y_train)\n",
        "# save the model\n",
        "dump(model, open('model.pkl','wb'))\n",
        "# save the scaler\n",
        "dump(scaler, open('scaler.pkl', 'wb'))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oK5WWWv7YWv2"
      },
      "source": [
        "# Load Model and Data Scaler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAR9FCXOYU3Q",
        "outputId": "1ba77473-eac3-442a-d110-d6af3a924ea9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# load model and scaler and make predictions on new data\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from pickle import load\n",
        "# prepare dataset\n",
        "X, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=1)\n",
        "# split data into train and test sets\n",
        "_, X_test, _, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
        "# load the model\n",
        "model = load(open('model.pkl','rb'))\n",
        "# load the scaler\n",
        "scaler = load(open('scaler.pkl', 'rb'))\n",
        "# check scale of the test set before scaling\n",
        "print('Raw test set range')\n",
        "for i in range(X_test.shape[1]):\n",
        "  print('>%d, min=%.3f, max=%.3f' %(i, X_test[:,i].min(), X_test[:,i].max()))\n",
        "# transform the test dataset\n",
        "X_test_scaled =scaler.transform(X_test)\n",
        "print('Scaled test set range')\n",
        "for i in range(X_test_scaled.shape[1]):\n",
        "  print('>%d, min=%.3f, max=%.3f'% (i, X_test_scaled[:, i].min(), X_test_scaled[:, i].max()))\n",
        "# make predictions on the test set\n",
        "yhat = model.predict(X_test_scaled)\n",
        "# evaluate accuracy\n",
        "acc = accuracy_score(y_test, yhat)\n",
        "print('Test Accuracy:', acc)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Raw test set range\n",
            ">0, min=-11.270, max=0.085\n",
            ">1, min=-5.581, max=5.926\n",
            "Scaled test set range\n",
            ">0, min=0.047, max=0.964\n",
            ">1, min=0.063, max=0.955\n",
            "Test Accuracy: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}