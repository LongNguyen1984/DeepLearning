{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPVahNUSxWjz2egMoW7N8vu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LongNguyen1984/DeepLearning/blob/master/Untitled7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9tiTdbYTl9b",
        "colab_type": "code",
        "outputId": "522355fc-ab0c-480b-ac1e-edd8f72bdde9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "\n",
        "data_path = '../drive/My Drive/data/'\n",
        "# importing image directly\n",
        "train_tensor = datasets.CIFAR10(data_path, train=True, download=True, transform=transforms.ToTensor())\n",
        "test_tensor = datasets.CIFAR10(data_path, train=False, download=True, transform=transforms.ToTensor())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fueQHlNG2VPz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# stacking tensors\n",
        "# 50000x(cxwxh)\n",
        "import torch\n",
        "imgs = torch.stack([img_t for img_t, _ in train_tensor], dim=3) # list.append in python\n",
        "\n",
        "mean = imgs.view(3,-1).mean(dim=1)\n",
        "std = imgs.view(3,-1).std(dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-dbIvnO0epN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "#Import transforms and apply RandomCrop for trainning data set\n",
        "from torchvision import transforms\n",
        "\n",
        "crop_size =20\n",
        "transformer = transforms.Compose([\n",
        "                                  #transforms.RandomCrop([crop_size,crop_size],padding= 5, pad_if_needed=True, fill=0, padding_mode='constant'), # apply random crop \n",
        "                                  transforms.RandomCrop([crop_size,crop_size]), # apply random crop \n",
        "                                  transforms.ToTensor(),# convert to tensor\n",
        "                                  transforms.Normalize(mean,std) # tensor to normalized tensor\n",
        "                                  ])\n",
        "\n",
        "transformed_train = datasets.CIFAR10(data_path, train=True, download=False,transform=transformer)\n",
        "transformed_test = datasets.CIFAR10(data_path, train=False, download=False,transform=transformer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ws8tLbEL4s57",
        "colab_type": "text"
      },
      "source": [
        "#Extract Dataset for 'Bird' and 'Airplane'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLYLqQ7r0vXT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this step will repeat random cropping\n",
        "label_map ={0:0, 2:1} # mapping label 0 -> 0, label 2 => 1\n",
        "#label_map ={0:[1.0,0.0], 2:[0.0, 1.0]} # mapping label 0 => 0, label 2 => 1\n",
        "class_names = {'airplane', 'bird'}\n",
        "train2 = [(img,label_map[label]) for img, label in transformed_train if label in [0,2]]\n",
        "test2 = [(img,label_map[label]) for img, label in transformed_test if label in [0,2]]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SRXGXDJ92Xf",
        "colab_type": "code",
        "outputId": "266dafa1-2b3b-400e-d655-7e52df205418",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "img, label = train2[64]\n",
        "#train2\n",
        "#label_map1 ={0:[1.0, 0.0], 1:[0.0, 1.0]}\n",
        "#for label in labels:\n",
        "#   print(label)\n",
        "label"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rXMlQGC6qIJ",
        "colab_type": "code",
        "outputId": "260365ff-e1f2-46e9-91ec-ff57eeb3fa3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "img, _ = train2[10]\n",
        "plt.imshow(img.permute(1, 2, 0))\n",
        "plt.show()"
      ],
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dS18hLCTDQdk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "81c80eea-6d18-401f-cf3e-3be7b0598651"
      },
      "source": [
        "img_batch = img.view(-1).unsqueeze(0)\n",
        "out = model(img_batch)\n",
        "out"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4994, 0.5006]], grad_fn=<SoftmaxBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsWmXCTWDbuq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c24174ce-e21d-473f-8a8f-8785e6763b9e"
      },
      "source": [
        "_, index = torch.max(out, dim=1)\n",
        "index.item()\n"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "si7x-jQ0II_Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2tKjR77kFCr",
        "colab_type": "text"
      },
      "source": [
        "#Build up the Training Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bimw2ssmys1A",
        "colab_type": "text"
      },
      "source": [
        "Just declare for the first time\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSzdxB05BxwD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# batch learning\n",
        "# epoch 1, ...., 50000 \n",
        "# batch (1~64) -> batch 1, (65~128) -> batch 2,\n",
        "# we usually shuffle the data for each epoch\n",
        "# SO, we may have different batch data for each epoch \n",
        "# enhance the effecitiveness of the parameter learning\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train2,batch_size=64,shuffle=True)#64\n",
        "param_size_input = 3*crop_size*crop_size\n",
        "# model = nn.Sequential(\n",
        "#     nn.Linear(3072,512),\n",
        "#     nn.Tanh(),\n",
        "#     nn.Linear(512,2),\n",
        "#     nn.LogSoftmax(dim=1)\n",
        "# )\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(param_size_input,1024),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(1024,512),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(512,128),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(128,2),\n",
        "    nn.Softmax(dim=1)\n",
        ")\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yr_kAPM20Mtn",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hZlHqa5kL40",
        "colab_type": "text"
      },
      "source": [
        "Tranning Model; Repeat serveral time when regenerate dataset through random cropping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lOyS_t3jt31",
        "colab_type": "code",
        "outputId": "f55ffe1a-91e5-4194-eede-af075f8b9a22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "label_map1 = {0:[1.0,0.0], 1:[0.0,1.0]}\n",
        "\n",
        "learning_rate=1e-2\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#loss_fn = nn.NLLLoss() # used with LogSoftMax Function -> loss function for classifcaiton\n",
        "loss_fn = nn.MSELoss()\n",
        "n_epochs = 100\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for imgs,labels in train_loader: # for each iteration, 64 random picture will be passed \n",
        "        batch_size = imgs.shape[0]\n",
        "        outputs = model(imgs.view(batch_size,-1)) # 64 picture -> model -> (64,2)\n",
        "        \n",
        "        var = []\n",
        "        for label in labels:\n",
        "          var.append(label_map1[label.item()])\n",
        "        var =torch.FloatTensor(var)\n",
        "        loss = loss_fn(outputs, var)\n",
        "        #loss = loss_fn(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
      ],
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss: 0.061916\n",
            "Epoch: 1, Loss: 0.013803\n",
            "Epoch: 2, Loss: 0.027146\n",
            "Epoch: 3, Loss: 0.045679\n",
            "Epoch: 4, Loss: 0.018507\n",
            "Epoch: 5, Loss: 0.034221\n",
            "Epoch: 6, Loss: 0.072852\n",
            "Epoch: 7, Loss: 0.043742\n",
            "Epoch: 8, Loss: 0.133404\n",
            "Epoch: 9, Loss: 0.067049\n",
            "Epoch: 10, Loss: 0.028419\n",
            "Epoch: 11, Loss: 0.039892\n",
            "Epoch: 12, Loss: 0.045490\n",
            "Epoch: 13, Loss: 0.007891\n",
            "Epoch: 14, Loss: 0.124880\n",
            "Epoch: 15, Loss: 0.007742\n",
            "Epoch: 16, Loss: 0.073878\n",
            "Epoch: 17, Loss: 0.022001\n",
            "Epoch: 18, Loss: 0.008315\n",
            "Epoch: 19, Loss: 0.005864\n",
            "Epoch: 20, Loss: 0.002045\n",
            "Epoch: 21, Loss: 0.074041\n",
            "Epoch: 22, Loss: 0.075924\n",
            "Epoch: 23, Loss: 0.017474\n",
            "Epoch: 24, Loss: 0.017662\n",
            "Epoch: 25, Loss: 0.006091\n",
            "Epoch: 26, Loss: 0.106221\n",
            "Epoch: 27, Loss: 0.029797\n",
            "Epoch: 28, Loss: 0.007443\n",
            "Epoch: 29, Loss: 0.006702\n",
            "Epoch: 30, Loss: 0.162079\n",
            "Epoch: 31, Loss: 0.013643\n",
            "Epoch: 32, Loss: 0.064246\n",
            "Epoch: 33, Loss: 0.007738\n",
            "Epoch: 34, Loss: 0.129102\n",
            "Epoch: 35, Loss: 0.080439\n",
            "Epoch: 36, Loss: 0.000788\n",
            "Epoch: 37, Loss: 0.021152\n",
            "Epoch: 38, Loss: 0.000859\n",
            "Epoch: 39, Loss: 0.016300\n",
            "Epoch: 40, Loss: 0.018701\n",
            "Epoch: 41, Loss: 0.063827\n",
            "Epoch: 42, Loss: 0.002426\n",
            "Epoch: 43, Loss: 0.064115\n",
            "Epoch: 44, Loss: 0.009811\n",
            "Epoch: 45, Loss: 0.050645\n",
            "Epoch: 46, Loss: 0.022383\n",
            "Epoch: 47, Loss: 0.133017\n",
            "Epoch: 48, Loss: 0.000587\n",
            "Epoch: 49, Loss: 0.000619\n",
            "Epoch: 50, Loss: 0.004805\n",
            "Epoch: 51, Loss: 0.000882\n",
            "Epoch: 52, Loss: 0.078696\n",
            "Epoch: 53, Loss: 0.065531\n",
            "Epoch: 54, Loss: 0.003266\n",
            "Epoch: 55, Loss: 0.001998\n",
            "Epoch: 56, Loss: 0.000086\n",
            "Epoch: 57, Loss: 0.064075\n",
            "Epoch: 58, Loss: 0.063438\n",
            "Epoch: 59, Loss: 0.004487\n",
            "Epoch: 60, Loss: 0.000280\n",
            "Epoch: 61, Loss: 0.062509\n",
            "Epoch: 62, Loss: 0.001105\n",
            "Epoch: 63, Loss: 0.001154\n",
            "Epoch: 64, Loss: 0.001315\n",
            "Epoch: 65, Loss: 0.000154\n",
            "Epoch: 66, Loss: 0.059378\n",
            "Epoch: 67, Loss: 0.008733\n",
            "Epoch: 68, Loss: 0.000296\n",
            "Epoch: 69, Loss: 0.001111\n",
            "Epoch: 70, Loss: 0.000376\n",
            "Epoch: 71, Loss: 0.000856\n",
            "Epoch: 72, Loss: 0.000973\n",
            "Epoch: 73, Loss: 0.062665\n",
            "Epoch: 74, Loss: 0.003643\n",
            "Epoch: 75, Loss: 0.001622\n",
            "Epoch: 76, Loss: 0.000164\n",
            "Epoch: 77, Loss: 0.001206\n",
            "Epoch: 78, Loss: 0.001643\n",
            "Epoch: 79, Loss: 0.000735\n",
            "Epoch: 80, Loss: 0.000474\n",
            "Epoch: 81, Loss: 0.000118\n",
            "Epoch: 82, Loss: 0.000106\n",
            "Epoch: 83, Loss: 0.003754\n",
            "Epoch: 84, Loss: 0.000446\n",
            "Epoch: 85, Loss: 0.000169\n",
            "Epoch: 86, Loss: 0.000332\n",
            "Epoch: 87, Loss: 0.058251\n",
            "Epoch: 88, Loss: 0.063090\n",
            "Epoch: 89, Loss: 0.000813\n",
            "Epoch: 90, Loss: 0.001149\n",
            "Epoch: 91, Loss: 0.000068\n",
            "Epoch: 92, Loss: 0.000862\n",
            "Epoch: 93, Loss: 0.062603\n",
            "Epoch: 94, Loss: 0.009575\n",
            "Epoch: 95, Loss: 0.000898\n",
            "Epoch: 96, Loss: 0.000249\n",
            "Epoch: 97, Loss: 0.000378\n",
            "Epoch: 98, Loss: 0.011187\n",
            "Epoch: 99, Loss: 0.000133\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNebD5EmFYO4",
        "colab_type": "code",
        "outputId": "198ddb8b-6708-4860-b81d-bfb1f2ba8916",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#def loss_fn(t_p, t_c):\n",
        "#  squared_diffs = (t_p - t_c)**2\n",
        "#  return squared_diffs.mean()\n",
        "a = ([1 ,0],[0,1.5])\n",
        "a1 = ([1,0],[0,1])\n",
        "a = torch.FloatTensor(a)\n",
        "a1 = torch.FloatTensor(a1)\n",
        "loss_fn = nn.MSELoss()\n",
        "loss_fn(a,a1)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0625)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aYI-Z5zGaIy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ynNRl_eDGrU",
        "colab_type": "code",
        "outputId": "2a008d5c-153e-4656-aa3d-0c97d22c5a3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#Convert labels to vector \n",
        "label_map1 = {0:[1.0,0.0], 1:[0.0,1.0]}\n",
        "var = []\n",
        "for label in labels:\n",
        "  var.append(label_map1[label.item()])\n",
        "  #var.append = ([ ])\n",
        "#label1 = torch.tensor([label_map1[label] for label in labels])\n",
        "#label_map1[]\n",
        "  #print(label.item())\n",
        "var =torch.FloatTensor(var)\n",
        "#loss_fn(outputs,var)\n",
        "outputs.size(), var.size()\n",
        "#print(torch.max(outputs,dim=1))\n",
        "loss_fn = nn.MSELoss()\n",
        "loss_fn(outputs,var)\n",
        "#outputs.item()\n",
        "#outputs, var\n",
        "#sum(torch.log(-outputs))"
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0638, grad_fn=<MseLossBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lEY379BERtw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSHSWYTvSQwl",
        "colab_type": "code",
        "outputId": "fe1bf125-d8cb-4d3b-e4b2-8387bd153b74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\n",
        "img_batch = img.view(-1).unsqueeze(0)\n",
        "imgs.shape[0]\n",
        "#outputs, labels"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVwrM7VwVfKa",
        "colab_type": "code",
        "outputId": "074b457c-27f6-4218-8270-703d15aeda13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "out = model(img_batch)\n",
        "out\n",
        "labels\n",
        "#torch.max(out,dim=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
              "        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n",
              "        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqDa9Vd6kKjj",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7m-IhZbLEKHa",
        "colab_type": "code",
        "outputId": "e72785b0-795b-40e8-e1f4-3e645be53b9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "val_loader = torch.utils.data.DataLoader(test2,batch_size=64,shuffle=False)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in val_loader:\n",
        "        batch_size = imgs.shape[0]\n",
        "        outputs = model(imgs.view(batch_size,-1)) # will generate probability \n",
        "        _,predicted = torch.max(outputs,dim=1) # we pick the category that has maximum probability\n",
        "        total += labels.shape[0] # true labels\n",
        "        correct += int( (predicted == labels).sum())\n",
        "\n",
        "print(\"Accuracy: %f\", correct/total)"
      ],
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: %f 0.776\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5pFgGkfj6uo",
        "colab_type": "code",
        "outputId": "8cb79b63-f193-40c6-8862-3aca8c870821",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "numel_list = [p.numel() for p in model.parameters() if p.requires_grad == True]\n",
        "sum(numel_list), numel_list"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3737474, [3145728, 1024, 524288, 512, 65536, 128, 256, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69fpW9MA8zQZ",
        "colab_type": "code",
        "outputId": "b948e3d6-2c49-47a7-d011-6f828f81d3ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "predicted, labels"
      ],
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0]),\n",
              " tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 184
        }
      ]
    }
  ]
}